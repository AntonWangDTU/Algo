{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7kEGGMRKQBdl"
   },
   "source": [
    "# SMM with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RDGR-iaV3vf1"
   },
   "source": [
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlFGxFZ997P4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L4zgNC-G3xwY"
   },
   "source": [
    "## Data Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE THE PATH TO YOUR COURSE DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u01OKdaOzoZE"
   },
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8Pb2U3lZ7Kz"
   },
   "outputs": [],
   "source": [
    "training_file = data_dir + \"SMM/A0201_training\"\n",
    "#training_file = data_dir + \"SMM/A2403_training\"\n",
    "\n",
    "training = np.loadtxt(training_file, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3u9HV5hztkh"
   },
   "source": [
    "### Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyp8DTSLzu4V"
   },
   "outputs": [],
   "source": [
    "evaluation_file = data_dir + \"SMM/A0201_evaluation\"\n",
    "#evaluation_file = data_dir + \"SMM/A2403_evaluation\"\n",
    "evaluation = np.loadtxt(evaluation_file, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uzw3jsGnzw0e"
   },
   "source": [
    "### Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYcVU4VLzyVe"
   },
   "outputs": [],
   "source": [
    "alphabet_file = data_dir + \"Matrices/alphabet\"\n",
    "alphabet = np.loadtxt(alphabet_file, dtype=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWEAPaggz0dJ"
   },
   "source": [
    "### Sparse Encoding Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecXsAxeMdYOX"
   },
   "outputs": [],
   "source": [
    "sparse_file = data_dir + \"Matrices/sparse\"\n",
    "_sparse = np.loadtxt(sparse_file, dtype=float)\n",
    "sparse = {}\n",
    "\n",
    "for i, letter_1 in enumerate(alphabet):\n",
    "    \n",
    "    sparse[letter_1] = {}\n",
    "\n",
    "    for j, letter_2 in enumerate(alphabet):\n",
    "        \n",
    "        sparse[letter_1][letter_2] = _sparse[i, j]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w2cQhiUW4QOL"
   },
   "source": [
    "## Peptide Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UhmQwUfqNzK"
   },
   "outputs": [],
   "source": [
    "def encode(peptides, encoding_scheme, alphabet):\n",
    "    \n",
    "    encoded_peptides = []\n",
    "\n",
    "    for peptide in peptides:\n",
    "\n",
    "        encoded_peptide = []\n",
    "\n",
    "        for peptide_letter in peptide:\n",
    "\n",
    "            for alphabet_letter in alphabet:\n",
    "\n",
    "                encoded_peptide.append(encoding_scheme[peptide_letter][alphabet_letter])\n",
    "\n",
    "        encoded_peptides.append(encoded_peptide)\n",
    "        \n",
    "    return np.array(encoded_peptides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww-p6gy81mqk"
   },
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hv2nx4Yq1lBf"
   },
   "outputs": [],
   "source": [
    "def cumulative_error(peptides, y, lamb, weights):\n",
    "\n",
    "    error = 0\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "\n",
    "        # get target prediction value\n",
    "        y_target = y[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "            \n",
    "        # calculate error\n",
    "        error += 1.0/2 * (y_pred - y_target)**2\n",
    "        \n",
    "    gerror = error + lamb*np.dot(weights, weights)\n",
    "    error /= len(peptides)\n",
    "        \n",
    "    return gerror, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict value for a peptide list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(peptides, weights):\n",
    "\n",
    "    pred = []\n",
    "    \n",
    "    for i in range(0, len(peptides)):\n",
    "        \n",
    "        # get peptide\n",
    "        peptide = peptides[i]\n",
    "        \n",
    "        # get prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "        \n",
    "        pred.append(y_pred)\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MSE between two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_mse(vec1, vec2):\n",
    "    \n",
    "    mse = 0\n",
    "    \n",
    "    for i in range(0, len(vec1)):\n",
    "        mse += (vec1[i] - vec2[i])**2\n",
    "        \n",
    "    mse /= len(vec1)\n",
    "    \n",
    "    return( mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfvPqSjL2g7u"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMXHiHmE2gh9"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon):\n",
    "    \n",
    "    # do is dE/dO\n",
    "    do = y_pred -y_target\n",
    "        \n",
    "    for i in range(0, len(weights)):\n",
    "        \n",
    "        de_dw_i = do*peptide[i] + 2*lamb_N*weights[i]\n",
    "\n",
    "        weights[i] -= epsilon*de_dw_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHXm8HAm4S_u"
   },
   "source": [
    "## Main Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1717
    },
    "colab_type": "code",
    "id": "EcHQYE2sja-y",
    "outputId": "3939a58c-88ac-4ae1-b680-edc4c2c913a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Gerr: 43.750889858236555 0.789509424400316 0.03539153117526048 0.7561347787039556 0.04016715130765897\n",
      "Epoch:  1 Gerr: 38.881527240605564 0.8154741496782673 0.031450990467899544 0.775952982487449 0.0375366520354359\n",
      "Epoch:  2 Gerr: 38.33507796855822 0.8189305466439499 0.03100834897326339 0.7856264299330675 0.036496495012562205\n",
      "Epoch:  3 Gerr: 37.44382689290254 0.822999683641678 0.030287592182902577 0.7881366496355203 0.035798702215261366\n",
      "Epoch:  4 Gerr: 37.36555418833412 0.8253682221197736 0.030223928157442112 0.7882464372390973 0.0366183666958875\n",
      "Epoch:  5 Gerr: 37.726763114721386 0.8221483914732047 0.030515684449826243 0.7770406248946663 0.038090917881541925\n",
      "Epoch:  6 Gerr: 37.625901764730045 0.8233191428658626 0.030434447242283818 0.79020891455352 0.03564695032045833\n",
      "Epoch:  7 Gerr: 37.046760025534894 0.825700495884668 0.029965808357565408 0.7869603546802211 0.03632217830321583\n",
      "Epoch:  8 Gerr: 37.21048718270839 0.8246092446126115 0.030098439864461292 0.7839461694311096 0.036659205724137355\n",
      "Epoch:  9 Gerr: 37.28745545281644 0.8238540953101806 0.030160650487877857 0.7880253829477567 0.03571523072985822\n",
      "Epoch:  10 Gerr: 37.124455809609174 0.8247863381514603 0.030028584259897483 0.7899506340939193 0.03567861130170519\n",
      "Epoch:  11 Gerr: 37.541291811027406 0.8228744141363814 0.03036569098277594 0.7823154114306251 0.037055202883755536\n",
      "Epoch:  12 Gerr: 37.462992000268805 0.8236696018549498 0.030302238910949655 0.7850587972518279 0.03682134784554733\n",
      "Epoch:  13 Gerr: 37.2894357727297 0.8245906810471 0.030162268466053774 0.7877487287472043 0.03611251141526469\n",
      "Epoch:  14 Gerr: 36.98640075305265 0.82537563409087 0.029917030708333955 0.7852697054904572 0.03630858108688396\n",
      "Epoch:  15 Gerr: 37.74828891604309 0.8221291129706458 0.030533460964462043 0.7828680310545663 0.03675237475325999\n",
      "Epoch:  16 Gerr: 37.272055654183916 0.8243523664110096 0.030147936910302233 0.7808747295399918 0.03737605741110484\n",
      "Epoch:  17 Gerr: 37.10577911200257 0.8249247436824932 0.03001332265836052 0.7853120849752624 0.03640757582868577\n",
      "Epoch:  18 Gerr: 37.115146896987724 0.8248141089645545 0.030021136261157592 0.7848164919077029 0.03632525579082353\n",
      "Epoch:  19 Gerr: 37.498116260507814 0.8237104437148887 0.030330782207188576 0.7821061994056888 0.03711800142536817\n",
      "Epoch:  20 Gerr: 36.91172415298205 0.8257201431461244 0.029856257009691748 0.789209622455157 0.03569386810294268\n",
      "Epoch:  21 Gerr: 37.71748838226894 0.8249701901027902 0.030507692293909188 0.7877211565195051 0.037070730547231145\n",
      "Epoch:  22 Gerr: 37.24885919238928 0.8253183882802358 0.0301286938205875 0.7877272432447204 0.03622365651208395\n",
      "Epoch:  23 Gerr: 37.778510840588496 0.822203720009598 0.030557591660496146 0.7833432774312544 0.0367461035544665\n",
      "Epoch:  24 Gerr: 37.51605160159492 0.8232544986665467 0.030345143843167114 0.7867991088401448 0.036212791956997406\n",
      "Epoch:  25 Gerr: 37.96931641393248 0.8232608339115577 0.0307114558585721 0.7848726039344339 0.037184928922335284\n",
      "Epoch:  26 Gerr: 37.40978494823322 0.8231290934941302 0.030259214794312896 0.7830894317282797 0.03662139957598873\n",
      "Epoch:  27 Gerr: 37.11579425519444 0.8256412230369938 0.0300211231951063 0.7832229767192149 0.037097616137418764\n",
      "Epoch:  28 Gerr: 36.83141208106069 0.8266827177156832 0.029791081814252204 0.7886989262337432 0.03623653426264831\n",
      "Epoch:  29 Gerr: 37.164298285308384 0.8259658352601534 0.030060702617438047 0.7860016294794377 0.036479949877820265\n",
      "Epoch:  30 Gerr: 37.19161728664685 0.8248631736627223 0.030082458558556728 0.7816126978347196 0.03724574316735004\n",
      "Epoch:  31 Gerr: 37.418793468294396 0.8239697198423417 0.030266545161216917 0.7848393879322983 0.03651095779892227\n",
      "Epoch:  32 Gerr: 37.41103073950702 0.8244446674348818 0.030260285328044305 0.7907878309461747 0.03551905284513636\n",
      "Epoch:  33 Gerr: 37.42182329939413 0.8231531581547935 0.03026892030481139 0.7745635292865759 0.037860891963889184\n",
      "Epoch:  34 Gerr: 37.145552065940514 0.825046951689881 0.030045300271024117 0.7903767480107909 0.03551483910863109\n",
      "Epoch:  35 Gerr: 37.13698760470675 0.8246583619615397 0.030038437885179167 0.7800941854214463 0.03702674876355393\n",
      "Epoch:  36 Gerr: 37.07911983452983 0.8253846403646744 0.029991621060773802 0.7851098076469105 0.03651390095376522\n",
      "Epoch:  37 Gerr: 37.148941777788856 0.8244690721334089 0.030048285516589914 0.7839234081778681 0.03633404305947358\n",
      "Epoch:  38 Gerr: 37.07973618339864 0.824932685872858 0.029992169413240385 0.7859891919893516 0.03612525555980289\n",
      "Epoch:  39 Gerr: 37.42527540904187 0.8241808148372446 0.03027149897610149 0.783321468846272 0.03698418348563589\n",
      "Epoch:  40 Gerr: 37.76212465101754 0.8238054704912089 0.030544303276780437 0.7891421711334791 0.03621360835931629\n",
      "Epoch:  41 Gerr: 37.306038222866526 0.824101671211781 0.030175269787060508 0.7871142825830659 0.03588783779725888\n",
      "Epoch:  42 Gerr: 38.08209183346319 0.8229497711005026 0.03080279430192734 0.795440786207473 0.03541629224537622\n",
      "Epoch:  43 Gerr: 37.056523043250245 0.8249641873966501 0.029973374926325916 0.7871969594893654 0.03587405256833883\n",
      "Epoch:  44 Gerr: 37.33042916657546 0.8240838626564848 0.03019499864722415 0.7872310780547452 0.03627909425717438\n",
      "Epoch:  45 Gerr: 37.45783456016351 0.8231256457558332 0.030298157921339823 0.7853041789536505 0.03644933761821187\n",
      "Epoch:  46 Gerr: 37.32930156230286 0.8238949752949574 0.03019410632224635 0.7777330138085915 0.03759960925350015\n",
      "Epoch:  47 Gerr: 37.593655258826324 0.8224805109633591 0.03040818090558966 0.7784249729980166 0.03714838182556601\n",
      "Epoch:  48 Gerr: 37.31186956751594 0.8240117533909751 0.030179886707790215 0.786835813949869 0.03620066862121667\n",
      "Epoch:  49 Gerr: 37.510233069195806 0.8229396987534093 0.0303404017873987 0.7835581222567004 0.03679493580771411\n",
      "Epoch:  50 Gerr: 37.18704578237317 0.8248719120887351 0.030078954236947543 0.781824008450446 0.03712477673214159\n",
      "Epoch:  51 Gerr: 37.391736850483625 0.8237412270051374 0.03024485278652259 0.7856939603450753 0.03637899554953267\n",
      "Epoch:  52 Gerr: 37.4288360864561 0.8235488936614055 0.030274696407406255 0.7851070301001213 0.03663689893157358\n",
      "Epoch:  53 Gerr: 37.044970960762 0.825230259760072 0.02996421090793762 0.786688334185333 0.03624263614144852\n",
      "Epoch:  54 Gerr: 37.50409516352935 0.8226114505548786 0.030335667285425687 0.7828164305149048 0.036566159444697516\n",
      "Epoch:  55 Gerr: 37.24631686830878 0.8247370398746889 0.030126929361339472 0.785459059686918 0.036546096908562795\n",
      "Epoch:  56 Gerr: 37.47540931454327 0.8241780362152992 0.030312237208035768 0.7926955746923222 0.03541042198177546\n",
      "Epoch:  57 Gerr: 37.64209006909084 0.8242439617759315 0.030447004425428723 0.7895535625474491 0.03598327895641348\n",
      "Epoch:  58 Gerr: 37.12436512751014 0.8247646035693409 0.030028302657988115 0.7878499069279613 0.03612964362882995\n",
      "Epoch:  59 Gerr: 37.77394075698872 0.8228427042515432 0.030553651150644314 0.7818818881577622 0.037351690893926696\n",
      "Epoch:  60 Gerr: 37.19943511054271 0.8244840090381623 0.030088952450370736 0.7877916065570452 0.036130066004829896\n",
      "Epoch:  61 Gerr: 37.431187837064165 0.8243302167223543 0.03027674364469002 0.7827229624886205 0.03680049182919889\n",
      "Epoch:  62 Gerr: 36.90512135084001 0.8259570604641548 0.029851081527048386 0.7862956284932288 0.03620394679264402\n",
      "Epoch:  63 Gerr: 37.0302025456866 0.8256602646084514 0.029952045144739942 0.7903657874552326 0.035794582159239904\n",
      "Epoch:  64 Gerr: 37.23050066126668 0.8242834936300392 0.03011415984558696 0.788642838746363 0.03598323051394352\n",
      "Epoch:  65 Gerr: 37.729860295350996 0.8255048987433968 0.030518472857732272 0.7856804812842012 0.036962846099522884\n",
      "Epoch:  66 Gerr: 37.38722148660337 0.8233323135526152 0.030241074768667643 0.7847856664378676 0.03635911650189827\n",
      "Epoch:  67 Gerr: 37.10358531548682 0.8248127290419935 0.03001170391517425 0.785117121739212 0.03624148732055379\n",
      "Epoch:  68 Gerr: 37.475820775477864 0.8242344991250707 0.030312586566868784 0.7863951135925558 0.036658854099266135\n",
      "Epoch:  69 Gerr: 37.83900944332298 0.8222605511901308 0.030606779224173895 0.7774852724774693 0.03739006197967747\n",
      "Epoch:  70 Gerr: 37.48049170435144 0.8231357824972555 0.030316302910059382 0.784515909812547 0.03658493613486581\n",
      "Epoch:  71 Gerr: 37.804304487995644 0.8211332941096268 0.030578221178517783 0.7817743443072115 0.03690166322532704\n",
      "Epoch:  72 Gerr: 37.46948145718725 0.8228397715648932 0.030307559466801054 0.7806954854887587 0.036919532205913444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  73 Gerr: 37.223707440911994 0.8242610500145835 0.030108645642991174 0.783916385595387 0.036536612796373505\n",
      "Epoch:  74 Gerr: 37.275601493512454 0.8242009698914099 0.030150685542240197 0.7884167078841742 0.03612685096875357\n",
      "Epoch:  75 Gerr: 37.74714063604701 0.8228978819261247 0.03053230421857428 0.7833091974215352 0.03679018074558156\n",
      "Epoch:  76 Gerr: 37.252060902723684 0.8247196739794663 0.030131621636265617 0.7813401760655061 0.03691801672488192\n",
      "Epoch:  77 Gerr: 36.97018717271422 0.8254167069346858 0.029903657907671227 0.7839617818368199 0.0364547518334043\n",
      "Epoch:  78 Gerr: 37.172002620754654 0.8248994297363409 0.030066840321228135 0.7922194214892135 0.03551829408595556\n",
      "Epoch:  79 Gerr: 37.427489291889046 0.8236458975542645 0.030273378409445287 0.7832426217248961 0.03691885013331995\n",
      "Epoch:  80 Gerr: 37.90594429705065 0.8220042539044226 0.030660590407057937 0.7808706397108056 0.03740996191760816\n",
      "Epoch:  81 Gerr: 37.129344074806376 0.8249533667178888 0.030031917458329276 0.7836305732177986 0.03691196003809778\n",
      "Epoch:  82 Gerr: 37.546050089125096 0.8241568670347703 0.030369251370522092 0.7887980699771994 0.036291632101202564\n",
      "Epoch:  83 Gerr: 37.08230499020436 0.8251366112539823 0.02999410887625899 0.786156832346336 0.0365001599225533\n",
      "Epoch:  84 Gerr: 36.860155311625064 0.8262668046679443 0.029814191321535236 0.7873836850296636 0.036254020536621655\n",
      "Epoch:  85 Gerr: 37.26020899062296 0.8239367144703191 0.03013784234580646 0.7786835874333597 0.037579749359672406\n",
      "Epoch:  86 Gerr: 37.621088779102216 0.8230442927863902 0.030429414809427496 0.7771217239964516 0.038226406415061066\n",
      "Epoch:  87 Gerr: 37.950179423580444 0.8242743771931255 0.03069598598298232 0.7870046227463185 0.03677100382620483\n",
      "Epoch:  88 Gerr: 37.69966405726902 0.8239325543746854 0.030493142735852595 0.7858507123067546 0.03685212054235875\n",
      "Epoch:  89 Gerr: 36.9535546672835 0.8255620983934487 0.029890139514514153 0.788594479030263 0.036047031580850916\n",
      "Epoch:  90 Gerr: 37.14057234291126 0.8248079043627817 0.030041274093739176 0.7888761738940694 0.0358365793484368\n",
      "Epoch:  91 Gerr: 37.177259901663696 0.824703972623091 0.030070792457848466 0.7906109008952825 0.035770869276131906\n",
      "Epoch:  92 Gerr: 37.79458431170418 0.8235395918645942 0.030570546207673886 0.7819857615030728 0.0371880024326885\n",
      "Epoch:  93 Gerr: 37.47958280567101 0.8235695660205209 0.030315548090210878 0.785777449575848 0.03637946344526297\n",
      "Epoch:  94 Gerr: 37.306035568104015 0.8253230746216871 0.030175041331967994 0.789924096530566 0.03589189668143615\n",
      "Epoch:  95 Gerr: 38.25880897950725 0.8229842679403591 0.030946377367379426 0.7830376848991524 0.037209530248339985\n",
      "Epoch:  96 Gerr: 37.91757265991697 0.8219405602486557 0.030670089664739827 0.7808924283509733 0.03727117138672202\n",
      "Epoch:  97 Gerr: 37.131555809587816 0.8250366964393964 0.030034014780144766 0.7878630084290721 0.036101904431626065\n",
      "Epoch:  98 Gerr: 37.43673909360349 0.8233028653858138 0.030280885445281683 0.7827035026629588 0.0367962823527177\n",
      "Epoch:  99 Gerr: 37.52611617488747 0.823490716617164 0.030353024504340288 0.7849890215316245 0.03687618317186189\n"
     ]
    }
   ],
   "source": [
    "# Random seed \n",
    "np.random.seed( 1 )\n",
    "\n",
    "# peptides\n",
    "peptides = training[:, 0]\n",
    "peptides = encode(peptides, sparse, alphabet)\n",
    "N = len(peptides)\n",
    "\n",
    "# target values\n",
    "y = np.array(training[:, 1], dtype=float)\n",
    "\n",
    "#evaluation peptides\n",
    "evaluation_peptides = evaluation[:, 0]\n",
    "evaluation_peptides = encode(evaluation_peptides, sparse, alphabet)\n",
    "\n",
    "#evaluation targets\n",
    "evaluation_targets = np.array(evaluation[:, 1], dtype=float)\n",
    "\n",
    "# weights\n",
    "input_dim  = len(peptides[0])\n",
    "output_dim = 1\n",
    "w_bound = 0.1\n",
    "weights = np.random.uniform(-w_bound, w_bound, size=input_dim)\n",
    "\n",
    "# training epochs\n",
    "epochs = 100\n",
    "\n",
    "# regularization lambda\n",
    "#lamb = 1\n",
    "lamb = 10\n",
    "#lamb = 0.01\n",
    "\n",
    "# regularization lambda per target value\n",
    "lamb_N = lamb/N\n",
    "\n",
    "# learning rate\n",
    "epsilon = 0.01\n",
    "\n",
    "# error  plot\n",
    "gerror_plot = []\n",
    "mse_plot = []\n",
    "train_mse_plot = []\n",
    "eval_mse_plot = []\n",
    "train_pcc_plot = []\n",
    "eval_pcc_plot = []\n",
    "\n",
    "# for each training epoch\n",
    "for e in range(0, epochs):\n",
    "\n",
    "    # for each peptide\n",
    "    for i in range(0, N):\n",
    "\n",
    "        # random index\n",
    "        ix = np.random.randint(0, N)\n",
    "        \n",
    "        # get peptide       \n",
    "        peptide = peptides[ix]\n",
    "\n",
    "        # get target prediction value\n",
    "        y_target = y[ix]\n",
    "       \n",
    "        # get initial prediction\n",
    "        y_pred = np.dot(peptide, weights)\n",
    "\n",
    "        # gradient descent \n",
    "        gradient_descent(y_pred, y_target, peptide, weights, lamb_N, epsilon)\n",
    "\n",
    "    # compute error\n",
    "    gerr, mse = cumulative_error(peptides, y, lamb, weights) \n",
    "    gerror_plot.append(gerr)\n",
    "    mse_plot.append(mse)\n",
    "    \n",
    "    # predict on training data\n",
    "    train_pred = predict( peptides, weights )\n",
    "    train_mse = cal_mse( y, train_pred )\n",
    "    train_mse_plot.append(train_mse)\n",
    "    train_pcc = pearsonr( y, train_pred )\n",
    "    train_pcc_plot.append( train_pcc[0] )\n",
    "        \n",
    "    # predict on evaluation data\n",
    "    eval_pred = predict(evaluation_peptides, weights )\n",
    "    eval_mse = cal_mse(evaluation_targets, eval_pred )\n",
    "    eval_mse_plot.append(eval_mse)\n",
    "    eval_pcc = pearsonr(evaluation_targets, eval_pred)\n",
    "    eval_pcc_plot.append( eval_pcc[0] )\n",
    "    \n",
    "    print (\"Epoch: \", e, \"Gerr:\", gerr, train_pcc[0], train_mse, eval_pcc[0], eval_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gN8BPXSm0HVy"
   },
   "source": [
    "## Error Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "Nca2qMKSOjGP",
    "outputId": "31d1c685-a0c2-4939-e34c-1961b744d686"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi= 80)\n",
    "\n",
    "x = np.arange(0, len(gerror_plot))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x, gerror_plot)\n",
    "plt.ylabel(\"Global Error\", fontsize=10);\n",
    "plt.xlabel(\"Iterations\", fontsize=10);\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x, mse_plot)\n",
    "plt.ylabel(\"MSE\", fontsize=10);\n",
    "plt.xlabel(\"Iterations\", fontsize=10);\n",
    "\n",
    "\n",
    "x = np.arange(0, len(train_mse_plot))\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x, train_mse_plot, label=\"Training Set\")\n",
    "plt.plot(x, eval_mse_plot, label=\"Evaluation Set\")\n",
    "plt.ylabel(\"Mean Squared Error\", fontsize=10);\n",
    "plt.xlabel(\"Iterations\", fontsize=10);\n",
    "plt.legend(loc='upper right');\n",
    "\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x, train_pcc_plot, label=\"Training Set\")\n",
    "plt.plot(x, eval_pcc_plot, label=\"Evaluation Set\")\n",
    "plt.ylabel(\"Pearson Correlation\", fontsize=10);\n",
    "plt.xlabel(\"Iterations\", fontsize=10);\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rs25DaDT0Wub"
   },
   "source": [
    "## Get PSSM Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6UDmJBpo0Y10"
   },
   "source": [
    "### Vector to Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Him7vVoh0arv"
   },
   "outputs": [],
   "source": [
    "# our matrices are vectors of dictionaries\n",
    "def vector_to_matrix(vector, alphabet):\n",
    "    \n",
    "    rows = int(len(vector)/len(alphabet))\n",
    "    \n",
    "    matrix = [0] * rows\n",
    "    \n",
    "    offset = 0\n",
    "    \n",
    "    for i in range(0, rows):\n",
    "        \n",
    "        matrix[i] = {}\n",
    "        \n",
    "        for j in range(0, 20):\n",
    "            \n",
    "            matrix[i][alphabet[j]] = vector[j+offset] \n",
    "        \n",
    "        offset += len(alphabet)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvrlgKrS0awJ"
   },
   "source": [
    "### Matrix to Psi-Blast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e63tgdna0cs3"
   },
   "outputs": [],
   "source": [
    "def to_psi_blast(matrix):\n",
    "\n",
    "    # print to user\n",
    "    \n",
    "    header = [\"\", \"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "    print('{:>4} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8}'.format(*header)) \n",
    "\n",
    "    letter_order = [\"A\", \"R\", \"N\", \"D\", \"C\", \"Q\", \"E\", \"G\", \"H\", \"I\", \"L\", \"K\", \"M\", \"F\", \"P\", \"S\", \"T\", \"W\", \"Y\", \"V\"]\n",
    "\n",
    "    for i, row in enumerate(matrix):\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        scores.append(str(i+1) + \" A\")\n",
    "\n",
    "        for letter in letter_order:\n",
    "\n",
    "            score = row[letter]\n",
    "\n",
    "            scores.append(round(score, 4))\n",
    "\n",
    "        print('{:>4} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8} {:>8}'.format(*scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlxa9_Ik0cxc"
   },
   "source": [
    "### Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "72AM0xu40iUV",
    "outputId": "0d4813af-41f5-418a-85c7-4dde29ead1c9"
   },
   "outputs": [],
   "source": [
    "matrix = vector_to_matrix(weights, alphabet)\n",
    "to_psi_blast(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eq3byUBi0OAT"
   },
   "source": [
    "## Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh7HCLo7PjKi"
   },
   "outputs": [],
   "source": [
    "evaluation_peptides = evaluation[:, 0]\n",
    "evaluation_peptides = np.array(encode(evaluation_peptides, sparse, alphabet))\n",
    "\n",
    "evaluation_targets = np.array(evaluation[:, 1], dtype=float)\n",
    "\n",
    "y_pred = []\n",
    "for i in range(0, len(evaluation_peptides)):\n",
    "    y_pred.append(np.dot(evaluation_peptides[i].T, weights))\n",
    "\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "lAPfkFcCRsuc",
    "outputId": "1c409c4d-af9a-4021-c9d8-e6f374098a1c"
   },
   "outputs": [],
   "source": [
    "pcc = pearsonr(evaluation_targets, np.array(y_pred))\n",
    "print(\"PCC: \", pcc[0])\n",
    "\n",
    "plt.scatter(y_pred, evaluation_targets);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "smm_gradient_descent_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
