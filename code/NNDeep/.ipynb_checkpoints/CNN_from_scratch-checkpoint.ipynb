{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7541a0f-92d5-4f5a-9d8d-f07c3e275d29",
   "metadata": {},
   "source": [
    "# Train a convolutional neural network to predict MHC ligands\n",
    "The notebook consists of the following sections:\n",
    "\n",
    "0. Module imports, define functions, set constants\n",
    "1. Load Data\n",
    "2. Build Model\n",
    "3. Select Hyper-paramerters\n",
    "4. Instantiate\n",
    "5. Train Model\n",
    "6. Evaluation\n",
    "\n",
    "## Exercise\n",
    "This exercise will be following the FFNN exercise closely. We will write a function that performs convolutions on a 1D input (i.e. a sequence) across the temporal dimension (i.e. the positions in the sequence). The outputs of the convolution will then be passed to an FFNN that will output a predicted binding affinity value as in the previous exercise. As we have already implemented the FFNN we only need to write the convolution function and then connect it with the FFNN code, using a global max pooling operation.\n",
    "\n",
    "The CNN architecture is more complex than the FFNN and consequently requires a more efficient implementation in order for it to not be slow to train. Particularly, here we will make use of NumPy's broadcasting functionalities, in order to reduce the number of slow for-loops and multiplication operations we have to make during the forward and backwards passes. You can read more about broadcasting here: https://numpy.org/doc/stable/user/basics.broadcasting.html. \n",
    "\n",
    "You should first implement the intuitive version of the CNN using for loops, ensure that your model trains and subsequently use the vectorized version for evaluation. You are of course also welcome to use your own implementation if you have time to wait for it to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "609c9fd0-5d6e-4249-8e96-d59a70403769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hostp\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe0f66",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffe395a-cf67-4124-ba10-696e33adcba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions you will re-use\n",
    "# Data-related utility functions\n",
    "def load_blosum(filename):\n",
    "    \"\"\"\n",
    "    Read in BLOSUM values into matrix.\n",
    "    \"\"\"\n",
    "    aa = ['A', 'R', 'N' ,'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'X']\n",
    "    df = pd.read_csv(filename, sep='\\s+', comment='#', index_col=0)\n",
    "    return df.loc[aa, aa]\n",
    "\n",
    "def load_peptide_target(filename):\n",
    "    \"\"\"\n",
    "    Read amino acid sequence of peptides and\n",
    "    corresponding log transformed IC50 binding values from text file.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep='\\s+', usecols=[0,1], names=['peptide','target'])\n",
    "    return df.sort_values(by='target', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def encode_peptides(X_in, blosum_file, max_pep_len=9):\n",
    "    \"\"\"\n",
    "    Encode AA seq of peptides using BLOSUM50.\n",
    "    Returns a tensor of encoded peptides of shape (1, max_pep_len, n_features) for a single batch\n",
    "    \"\"\"\n",
    "    blosum = load_blosum(blosum_file)\n",
    "    \n",
    "    batch_size = len(X_in)\n",
    "    n_features = len(blosum)\n",
    "    \n",
    "    X_out = np.zeros((batch_size, max_pep_len, n_features), dtype=np.int8)\n",
    "    \n",
    "    for peptide_index, row in X_in.iterrows():\n",
    "        for aa_index in range(len(row.peptide)):\n",
    "            X_out[peptide_index, aa_index] = blosum[ row.peptide[aa_index] ].values\n",
    "            \n",
    "    return X_out, np.expand_dims(X_in.target.values,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "226a7fef-b833-4c37-86b1-b65e20c25db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc. functions\n",
    "def invoke(early_stopping, loss, model, implement=False):\n",
    "    if implement == False:\n",
    "        return False\n",
    "    else:\n",
    "        early_stopping(loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            return True\n",
    "        \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def plot_losses(train_losses, valid_losses, n_epochs):\n",
    "    # Plotting the losses \n",
    "    fig,ax = plt.subplots(1,1, figsize=(9,5))\n",
    "    ax.plot(range(n_epochs), train_losses, label='Train loss', c='b')\n",
    "    ax.plot(range(n_epochs), valid_losses, label='Valid loss', c='m')\n",
    "    ax.legend()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6ca797-4889-41af-a5d0-8a1b47e96ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving and loading functions\n",
    "\n",
    "def save_cnn_model(filepath, model):\n",
    "    if not filepath.endswith('.pkl'):\n",
    "        filepath = filepath+'.pkl'\n",
    "    with open(filepath, 'wb') as f:\n",
    "        dict_to_save = {'filter_size': model.filter.shape[0], 'input_size': model.filter.shape[1], 'n_filters': model.filter.shape[2], 'ffnn_hidden_size':model.ffnn.W1.shape[1], 'output_size':model.ffnn.W2.shape[1],\n",
    "                        'W1': model.ffnn.W1, 'b1':model.ffnn.b1, 'W2':model.ffnn.W2, 'b2':model.ffnn.b2, \"filter\": model.filter,}\n",
    "        pickle.dump(dict_to_save, f)\n",
    "        print(f'Saved CNN model at {filepath}')\n",
    "\n",
    "\n",
    "def load_cnn_model(filepath, model=None):\n",
    "\n",
    "    with open(filepath, 'rb') as f:\n",
    "        loaded_dict = pickle.load(f)\n",
    "    if model is None:\n",
    "            model = SimpleCNN_vectorized(loaded_dict['filter_size'], loaded_dict['input_size'], loaded_dict['n_filters'], loaded_dict['ffnn_hidden_size'], loaded_dict['output_size'])\n",
    "    assert (model.filter.shape[0]==loaded_dict['filter_size'] and model.filter.shape[1]==loaded_dict['input_size'] and model.filter.shape[2]==loaded_dict['n_filters'] and model.ffnn.W1.shape[1]==loaded_dict['ffnn_hidden_size'] and model.ffnn.W2.shape[1]==loaded_dict['output_size']), \\\n",
    "        f\"Model and loaded weights size mismatch!. Provided model has weight of dimensions {model.ffnn.W1.shape, model.ffnn.W2.shape, model.filter.shape} ; Loaded weights have shape {loaded_dict['W1'].shape, loaded_dict['W2'].shape, loaded_dict['filter'].shape}\"\n",
    "\n",
    "    model.ffnn.W1 = loaded_dict['W1']\n",
    "    model.ffnn.b1 = loaded_dict['b1']\n",
    "    model.ffnn.W2 = loaded_dict['W2']\n",
    "    model.ffnn.b2 = loaded_dict['b2']\n",
    "    model.filter = loaded_dict['filter']\n",
    "    print(f\"Model loaded successfully from {filepath}\\nwith weights [ W1, W2, filter] dimensions : {model.ffnn.W1.shape, model.ffnn.W2.shape, model.filter.shape}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44979f18-9c57-400e-8cee-45ec044007bb",
   "metadata": {},
   "source": [
    "# Data loading and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98cb9e0a-3c81-46cf-997e-725ff47d67ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the dataframe ; Peptides have to be *encoded* to BLOSUM matrices\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATSTRHPSK</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RSYSPRNSR</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALYYVHSLLY</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LVKSSFVKK</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VTFRERYSYK</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      peptide  target\n",
       "0   ATSTRHPSK     1.0\n",
       "1   RSYSPRNSR     1.0\n",
       "2  ALYYVHSLLY     1.0\n",
       "3   LVKSSFVKK     1.0\n",
       "4  VTFRERYSYK     1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_datapoints:\n",
      "Train data:\t 3951\n",
      "Valid data:\t 1329\n",
      "Test data:\t 1321\n",
      "Maximum peptide length of each data set:\n",
      "Train:\t 11\n",
      "Valid:\t 11\n",
      "Test:\t 11\n"
     ]
    }
   ],
   "source": [
    "# Replace your data paths with the actual paths and desired alleles\n",
    "ALLELE =  'A0301' #'A0201'\n",
    "DATAPATH = '../../data/NNDeep/'\n",
    "savepath = DATAPATH + \"prefix\" # NOTE: Change this to your desired save path when converting to scripts\n",
    "blosum_file = f'{DATAPATH}/BLOSUM50'\n",
    "train_data = f'{DATAPATH}/{ALLELE}/train_BA'\n",
    "valid_data = f'{DATAPATH}/{ALLELE}/valid_BA'\n",
    "test_data = f'{DATAPATH}/{ALLELE}/test_BA'\n",
    "\n",
    "# Loading the peptides.\n",
    "train_raw = load_peptide_target(train_data)\n",
    "valid_raw = load_peptide_target(valid_data)\n",
    "test_raw = load_peptide_target(test_data)\n",
    "# \n",
    "print('Preview of the dataframe ; Peptides have to be *encoded* to BLOSUM matrices')\n",
    "display(train_raw.head())\n",
    "\n",
    "print('N_datapoints:')\n",
    "print('Train data:\\t', train_raw.shape[0])\n",
    "print('Valid data:\\t', valid_raw.shape[0])\n",
    "print('Test data:\\t', test_raw.shape[0])\n",
    "\n",
    "print('Maximum peptide length of each data set:')\n",
    "print('Train:\\t',  train_raw['peptide'].apply(len).max())\n",
    "print('Valid:\\t', valid_raw['peptide'].apply(len).max())\n",
    "print('Test:\\t', test_raw['peptide'].apply(len).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108093e-4d84-4a96-808c-f97b424f8d5c",
   "metadata": {},
   "source": [
    "Peptide encoding : \n",
    "\n",
    "We need to ensure that every peptide shorter than the maximum length `max_len_pep` are *padded* to that length when building the matrices, in order for the dimensions to fit. For example, if our `max_pep_len` is 11, then every peptide of length shorter than 11 must be padded to 11. \n",
    "\n",
    "For example, for a peptide \"GILGFVFTL\", of size 9, and a `max_pep_len` of 11, this effectively means that the peptide is first padded to 11 : \"GILGFVFTLXX\", where \"x\" means a pad, and then converted to a matrix. Note here, that the CNN can take a sequence of arrays as input and we no longer need to flatten the input array along the encoding dimension. Thus the X array will be 3-dimensional with dimensions (data_length, peptide_length, encoding_length).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c144f09-7d56-46fc-a857-609c9b18f8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3951, 11, 21)\n"
     ]
    }
   ],
   "source": [
    "max_pep_len = train_raw.peptide.apply(len).max()\n",
    "x_train_, y_train_ = encode_peptides(train_raw, blosum_file, max_pep_len)\n",
    "x_valid_, y_valid_ = encode_peptides(valid_raw, blosum_file, max_pep_len)\n",
    "x_test_, y_test_ = encode_peptides(test_raw, blosum_file, max_pep_len)\n",
    "\n",
    "# We now have matrices of shape (N_datapoints, max_pep_len, n_features)\n",
    "print(x_train_.shape)\n",
    "#Observations, maximum_nr of peptides, nr of features: 20 AA's and x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21fb20-f0b9-43be-8ac3-0b6c7ca26f58",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "Now it's your turn. We are using a class structure, where a class contains `methods` needed for the forward pass (that just means functions that belong to a class) and you will code the derivatives and backpropagation separately.\n",
    "\n",
    "For the forward pass, you will need to define the shape of your weight and bias matrices, as well as the actual `forward` pass in terms of matrix multiplications. Additionally, as we are using neural networks, you will also need to code the activation functions (ReLU and Sigmoid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de78ebb7-eb36-4551-8fb9-dbc582e9c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights initialization function.\n",
    "# xavier initialization is technically more stable and preferred \n",
    "# (See slides)\n",
    "def xavier_initialization_normal(filter_size, input_size, n_filters):\n",
    "    shape = (filter_size, input_size, n_filters)\n",
    "    fan_in = filter_size * input_size\n",
    "    fan_out = filter_size * n_filters\n",
    "    stddev = np.sqrt(2 / (fan_in + fan_out))\n",
    "    return np.random.normal(0, stddev, size=shape) * 0.1\n",
    "\n",
    "def xavier_initialization_normal_ffnn(input_dim, output_dim):\n",
    "    shape = (input_dim, output_dim)\n",
    "    stddev = np.sqrt(2 / (input_dim + output_dim))\n",
    "    return np.random.normal(0, stddev, size=shape) * 0.1\n",
    "\n",
    "def random_initialization_normal(input_dim, output_dim):\n",
    "    return np.random.randn(input_dim, output_dim) * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9284a-cf6e-4184-a2db-571aaf4b8053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(net, x_train, y_train, learning_rate):\n",
    "    \"\"\"\n",
    "    Trains the network for a single epoch, running the forward and backward pass, and compute and return the loss.\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    conv_output, pooled_output, pool_indices, relu_output, z1, a1, z2, a2,  = net.forward(x_train)\n",
    "    # backward pass\n",
    "    backward(net, x_train, y_train, conv_output, pooled_output, pool_indices, relu_output, z1, a1, z2, a2, learning_rate)\n",
    "    loss = np.mean((a2 - y_train) ** 2)\n",
    "    return loss\n",
    "        \n",
    "def eval_network(net, x_valid, y_valid):\n",
    "    \"\"\"\n",
    "    Evaluates the network ; Note that we do not update weights (no backward pass)\n",
    "    \"\"\"\n",
    "    conv_output, pooled_output, pool_indices, relu_output, z1, a1, z2, a2, = net.forward(x_valid)\n",
    "    loss = np.mean((a2-y_valid)**2)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd57a3c-79a9-486e-aaad-79efe15a119d",
   "metadata": {},
   "source": [
    "# Now create a model and run it.\n",
    "\n",
    "Play around with the hyperparameters (number of epochs, learning rate, hidden size) and see what the changes do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70063caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN part\n",
    "class SimpleFFNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases with small random values\n",
    "        self.W1 = xavier_initialization_normal_ffnn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros(hidden_size)\n",
    "        self.W2 = xavier_initialization_normal_ffnn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros(output_size)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def sigmoid(self, x): \n",
    "        # This is equivalent to : \n",
    "        # if x>=0, then compute (1/(1+np.exp(-x)))\n",
    "        # else: compute (np.exp(x)/(1+np.exp(x))))\n",
    "        return np.where(x >= 0, 1 / (1 + np.exp(-x)), \n",
    "                        np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        zi denotes the output of a hidden layer i\n",
    "        ai denotes the output of an activation function at layer i\n",
    "        (activations are relu, sigmoid, tanh, etc.)\n",
    "        \"\"\"\n",
    "\n",
    "        # First layer\n",
    "        z1 = np.dot(x, self.W1) + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        \n",
    "        # Output layer\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        a2 = self.sigmoid(z2)\n",
    "        \n",
    "        # Return all the intermediate outputs as well because we need them for backpropagation (see slides)\n",
    "        return z1, a1, z2, a2\n",
    "\n",
    "\n",
    "class SimpleCNN:\n",
    "    def __init__(self, filter_size, input_size, n_filters, hidden_size, output_size):\n",
    "        self.filter = xavier_initialization_normal(filter_size, input_size, n_filters)\n",
    "        self.bias = np.zeros((1, 1, n_filters))\n",
    "        self.ffnn = SimpleFFNN(n_filters, hidden_size, output_size)\n",
    "\n",
    "    def conv1d(self, batch_x):\n",
    "        batch_size, input_length, input_size = batch_x.shape\n",
    "        filter_size, _, n_filters = self.filter.shape\n",
    "        output_length = input_length - filter_size + 1\n",
    "        batch_output = np.zeros((batch_size, output_length, n_filters))\n",
    "\n",
    "        # Loop through each sample in the batch\n",
    "        for b in range(batch_size):\n",
    "            for i in range(output_length):\n",
    "                for k in range(n_filters):\n",
    "                    # Extract the slice of input for the current position. Hint: we need to slice batch_x like [b, XX, :]\n",
    "                    input_slice = batch_x[b, i:i + filter_size, :] #XX\n",
    "                    # Do element-wise multiplication of input slice filter k and sum. Hint: we need to slice the filter tensor and bias like this [:, :, k]\n",
    "                    batch_output[b, i, k] = np.sum(self.filter[:,:,k] * input_slice) + self.bias[:,:,k] #XX\n",
    "\n",
    "        return batch_output\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def global_max_pooling(self, x):\n",
    "        # Find the maximum value for each feature map and its index. Hint: look at np.max and np.argmax\n",
    "        pool_indices = np.argmax(x, axis = 1) #XX\n",
    "        max_pool = np.max(x, axis = 1) #XX\n",
    "        return max_pool, pool_indices\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolution layer\n",
    "        conv_output = self.conv1d(x)\n",
    "        # Apply global max pooling\n",
    "        pooled_outout, pool_indices = self.global_max_pooling(conv_output)\n",
    "        # Apply the ReLU activation function\n",
    "        relu_output = self.relu(pooled_outout)\n",
    "        \n",
    "        # Pass the result through the feedforward neural network. Hint: unroll the output of the forward function like this *XX\n",
    "        return conv_output, pooled_outout, pool_indices, relu_output, *self.ffnn.forward(relu_output)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "def backward(net, x, y, conv_output, pooled_output, pool_indices, relu_output, z1, a1, z2, a2, learning_rate=0.01):\n",
    "    # Backwards pass for the FFNN\n",
    "    error = a2 - y\n",
    "    d_output = error * sigmoid_derivative(a2)\n",
    "\n",
    "    d_W2 = np.dot(a1.T, d_output)\n",
    "    d_b2 = np.sum(d_output, axis=0, keepdims=True)\n",
    "\n",
    "    error_hidden_layer = np.dot(d_output, net.ffnn.W2.T)\n",
    "    d_hidden_layer = error_hidden_layer * relu_derivative(a1)\n",
    "\n",
    "    d_W1 = np.dot(relu_output.T, d_hidden_layer)\n",
    "    d_b1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "    # Propagate the error back to the pooled layer. Hint: look at how we calculated the gradients for the previous layers.\n",
    "    error_pool = np.dot(d_hidden_layer, net.ffnn.W1.T) #XX\n",
    "    d_pool = error_pool * relu_derivative(relu_output) #XX\n",
    "\n",
    "    # Propagate the error to the convolution\n",
    "    batch_size, input_length, input_size = x.shape\n",
    "    filter_size, input_size, n_filters = net.filter.shape\n",
    "    d_conv = np.zeros_like(conv_output)\n",
    "    for b in range(batch_size):\n",
    "        # Loop through each output channel\n",
    "        for k in range(n_filters):\n",
    "            # Get the index of the maximum value in the pooled output for this sample and channel. Hint: we kept track of this with pool_indices.\n",
    "            max_index = pool_indices[b, k] #XX\n",
    "            # Fill out the d_conv matrix at the max indices using the d_pool value for input b and filter k\n",
    "            d_conv[b, max_index, k] = d_pool[b, k] #XX\n",
    "    d_bias = np.sum(d_conv, axis=(0, 1), keepdims=True)\n",
    "\n",
    "    # Calculate the gradient for the convolution filter. \n",
    "    d_filter = np.zeros_like(net.filter)\n",
    "    output_length = input_length - filter_size + 1\n",
    "    for b in range(batch_size):\n",
    "        # Loop through each position in the convolution output\n",
    "        for i in range(output_length):\n",
    "            # Extract the input slice corresponding to the current position. Hint: we need to slice x like [b, XX, :]\n",
    "            input_slice = x[b, i:i + output_length, :]\n",
    "            # Loop through each output channel\n",
    "            for k in range(n_filters):\n",
    "                # Accumulate the gradient for the filter by multiplying the input slice with the corresponding gradient from d_conv.\n",
    "                # Hint for slicing d_conv: look at the dimensions of d_conv and which values are used in the loops.\n",
    "                d_filter[:, :, k] += np.sum(d_conv[b, i, k] * input_slice)\n",
    "\n",
    "    # Update the filter weights and the weights and biases of the FFNN\n",
    "    net.filter -= learning_rate * d_filter #XX\n",
    "    net.bias -= learning_rate * d_bias.squeeze() #XX\n",
    "    net.ffnn.W1 -= learning_rate * d_W1\n",
    "    net.ffnn.b1 -= learning_rate * d_b1.squeeze()\n",
    "    net.ffnn.W2 -= learning_rate * d_W2\n",
    "    net.ffnn.b2 -= learning_rate * d_b2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4166f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Find the index of the maximum value in the entire array\n",
    "max_index = np.argmax(a)\n",
    "print(\"Index of max value:\", max_index)  # Output: 8\n",
    "\n",
    "# Find the indices of the maximum values along axis 0 (column-wise)\n",
    "max_index_axis0 = np.argmax(a, axis=0)\n",
    "print(\"Indices of max values along axis 0:\", max_index_axis0)  # Output: [2 2 2]\n",
    "\n",
    "# Find the indices of the maximum values along axis 1 (row-wise)\n",
    "max_index_axis1 = np.argmax(a, axis=1)\n",
    "print(\"Indices of max values along axis 1:\", max_index_axis1)  # Output: [2 2 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_vectorized:\n",
    "    def __init__(self, filter_size, input_size, n_filters, hidden_size, output_size):\n",
    "        self.filter = xavier_initialization_normal(filter_size, input_size, n_filters)\n",
    "        self.bias = np.zeros((1, 1, n_filters))\n",
    "        self.ffnn = SimpleFFNN(n_filters, hidden_size, output_size)\n",
    "\n",
    "    def conv1d(self, batch_x):\n",
    "        batch_size, input_length, input_size = batch_x.shape\n",
    "        filter_size, _, n_filters = self.filter.shape\n",
    "        output_length = input_length - filter_size + 1\n",
    "        batch_output = np.zeros((batch_size, output_length, n_filters))\n",
    "\n",
    "        # Apply filters to each input slice of the input tensor\n",
    "        for i in range(output_length):\n",
    "            input_slice = batch_x[:, i:i + filter_size, :, None]  # Shape: (batch_size, filter_size, input_size, 1)\n",
    "            batch_output[:, i, :] = np.sum(input_slice * self.filter, axis=(1, 2)) + self.bias\n",
    "    \n",
    "        return batch_output\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def global_max_pooling(self, x):\n",
    "        # Find the maximum value for each feature map and its index. Hint: look at np.max and np.argmax\n",
    "        pool_indices = np.argmax(x, axis=1)\n",
    "        max_pool = np.max(x, axis=1)\n",
    "        return max_pool, pool_indices\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the convolution layer\n",
    "        conv_output = self.conv1d(x)\n",
    "        # Apply global max pooling\n",
    "        pooled_output, pool_indices = self.global_max_pooling(conv_output)\n",
    "        # Apply the ReLU activation function\n",
    "        relu_output = self.relu(pooled_output)\n",
    "        \n",
    "        # Pass the result through the feedforward neural network. Hint: unroll the output of the forward function like this *XX\n",
    "        return conv_output, pooled_output, pool_indices, relu_output, *self.ffnn.forward(relu_output)\n",
    "\n",
    "\n",
    "def backward(net, x, y, conv_output, pooled_output, pool_indices, relu_output, z1, a1, z2, a2, learning_rate=0.01):\n",
    "    # Backwards pass for the FFNN\n",
    "    error = a2 - y\n",
    "    d_output = error * sigmoid_derivative(a2)\n",
    "\n",
    "    d_W2 = np.dot(a1.T, d_output)\n",
    "    d_b2 = np.sum(d_output, axis=0, keepdims=True)\n",
    "\n",
    "    error_hidden_layer = np.dot(d_output, net.ffnn.W2.T)\n",
    "    d_hidden_layer = error_hidden_layer * relu_derivative(a1)\n",
    "\n",
    "    d_W1 = np.dot(relu_output.T, d_hidden_layer)\n",
    "    d_b1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "    # Propagate the error back to the pooled layer\n",
    "    error_pool = np.dot(d_hidden_layer, net.ffnn.W1.T)\n",
    "    d_pool = error_pool * relu_derivative(pooled_output)\n",
    "\n",
    "    # Propagate the error to the convolution\n",
    "    batch_size, input_length, input_size = x.shape\n",
    "    filter_size, input_size, n_filters = net.filter.shape\n",
    "    d_conv = np.zeros_like(conv_output)\n",
    "    batch_indices = np.arange(len(x))[:, None]\n",
    "    d_conv[batch_indices, pool_indices, :] = d_pool[:, None, :]\n",
    "    d_bias = np.sum(d_conv, axis=(0, 1), keepdims=True)\n",
    "\n",
    "    # Calculate the gradient for the convolution filter.\n",
    "    d_filter = np.zeros_like(net.filter)\n",
    "    output_length = input_length - filter_size + 1\n",
    "    for i in range(output_length):\n",
    "        # Extract slices for all batches and all positions\n",
    "        input_slice_batch = x[:, i:i + filter_size, :, None]  # Shape: (batch_size, filter_size, input_size, 1)\n",
    "        d_conv_slice = d_conv[:, i, None, :]  # Shape: (batch_size, 1, n_filters)\n",
    "\n",
    "        # Correct the shape for broadcasting\n",
    "        d_conv_slice = d_conv_slice[:, :, None, :]  # Shape: (batch_size, 1, 1, n_filters)\n",
    "\n",
    "        # Accumulate gradients\n",
    "        d_filter += np.sum(input_slice_batch * d_conv_slice, axis=0)  # Sum over batch_size axis\n",
    "\n",
    "    # Update the filter weights and the weights and biases of the FFNN\n",
    "    net.filter -= learning_rate * d_filter\n",
    "    net.bias -= learning_rate * d_bias.squeeze()\n",
    "    net.ffnn.W1 -= learning_rate * d_W1\n",
    "    net.ffnn.b1 -= learning_rate * d_b1.squeeze()\n",
    "    net.ffnn.W2 -= learning_rate * d_W2\n",
    "    net.ffnn.b2 -= learning_rate * d_b2.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff1b8b-0462-4e01-96cb-469fcdd44b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sizes\n",
    "input_size = x_train_.shape[2] # also known as \"n_features\"\n",
    "\n",
    "# Model and training hyperparameters\n",
    "learning_rate = 0.0001\n",
    "hidden_units = 50\n",
    "n_filters = 16\n",
    "filter_size = 5\n",
    "n_epochs = 500\n",
    "output_size = 1\n",
    "# Creating a model instance \n",
    "\n",
    "# Neural Network training here\n",
    "#network = SimpleCNN(filter_size=filter_size, input_size=input_size, n_filters=n_filters, \n",
    "#                    hidden_size=hidden_units, output_size=output_size)\n",
    "\n",
    "# Vectorized version\n",
    "network = SimpleCNN_vectorized(filter_size=filter_size, input_size=input_size, n_filters=n_filters, \n",
    "                     hidden_size=hidden_units, output_size=output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70c6d1-86f2-4ffe-9deb-bd8ef2ee8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loops\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# Run n_epochs of training\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_network(network, x_train_, y_train_, learning_rate)\n",
    "    valid_loss = eval_network(network, x_valid_, y_valid_)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    # For the first, every 1% of the epochs and last epoch, we print the loss \n",
    "    # to check that the model is properly training. (loss going down)\n",
    "    if (n_epochs >= 10 and epoch % math.ceil(0.01 * n_epochs) == 0) or epoch == 0 or epoch == n_epochs:\n",
    "        print(f\"Epoch {epoch}: \\n\\tTrain Loss:{train_loss:.4f}\\tValid Loss:{valid_loss:.4f}\")\n",
    "\n",
    "# saving the model to a file\n",
    "save_cnn_model(f'{savepath}_saved_cnn.pkl', model=network)\n",
    "\n",
    "# plotting the losses \n",
    "plot_losses(train_losses, valid_losses, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ba8f3-35c4-454c-b910-c0cc9506fc9a",
   "metadata": {},
   "source": [
    "# Evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0da2d-e602-4feb-9ef4-451f7b8e370a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO : Add evaluation / prediction on test data \n",
    "#        and get the AUC and other stats plots\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Reload the model and evaluate it\n",
    "reloaded_network = load_cnn_model(f'{savepath}_saved_cnn.pkl', model=network)\n",
    "\n",
    "# Thresholding the targets\n",
    "BINDER_THRESHOLD=0.426\n",
    "y_test_thresholded = (y_test_>=BINDER_THRESHOLD).astype(int)\n",
    "_, _, _, _, _, _, _, test_predictions = reloaded_network.forward(x_test_)\n",
    "\n",
    "# Saving the predictions\n",
    "test_raw['predictions'] = test_predictions\n",
    "test_raw[['peptide','predictions','target']].to_csv(f'{savepath}_ffnn_predictions.txt', index=False, header=False)\n",
    "\n",
    "# Plot ROC curve\n",
    "test_auc = roc_auc_score(y_test_thresholded.squeeze(), test_predictions.squeeze())\n",
    "test_fpr, test_tpr, _ = roc_curve(y_test_thresholded.squeeze(), test_predictions.squeeze())\n",
    "\n",
    "f,a = plt.subplots(1,1 , figsize=(9,9))\n",
    "\n",
    "a.plot([0,1],[0,1], ls=':', lw=0.5, label='Random prediction: AUC=0.500', c='k')\n",
    "a.plot(test_fpr, test_tpr, ls='--', lw=1, label=f'Neural Network: AUC={test_auc:.3f}', c='b')\n",
    "a.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceaef7f",
   "metadata": {},
   "source": [
    "# Argparse part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03947ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to make two scripts train_ffnn.py ; test_ffnn.py: \n",
    "#     One for training and saving a model\n",
    "#     One for loading a saved model and predict\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# Train part\n",
    "parser = ArgumentParser(description=\"FFNN train script\")\n",
    "# Data paths\n",
    "parser.add_argument(\"-train\", action=\"store\", dest=\"train_data\", type=str, help=\"File with peptides (pep target)\")\n",
    "parser.add_argument(\"-valid\", action=\"store\", dest=\"valid_data\", type=str, help=\"File with peptides (pep target)\")\n",
    "parser.add_arguemnt(\"-savepath\", action=\"store\", dest=\"savepath\", type=str, default='./CustomName', help='Path to save the result. Used to save the model as {savepath}_ffnn_model.pkl Must not have an extension, ex : ./path/to/my_file')\n",
    "\n",
    "# Model parameters\n",
    "parser.add_argument(\"-nh\", action=\"store\", dest=\"n_hidden\", type=int, default=16, help=\"Number of hidden units\")\n",
    "parser.add_argument(\"-nf\", action=\"store\", dest=\"n_filters\", type=int, default=16, help=\"Number of CNN filters\")\n",
    "parser.add_argument(\"-fs\", action=\"store\", dest=\"filter_size\", type=int, default=5, help=\"CNN kernel size\")\n",
    "parser.add_argument(\"-ne\", action=\"store\", dest=\"n_epochs\", type=int, default=500, help=\"Number of epochs\")\n",
    "parser.add_argument(\"-lr\", action=\"store\", dest=\"learning_rate\", type=float, default=0.0001, help=\"Learning rate\")\n",
    "args = parser.parse_args()\n",
    "train_data = parser.train_data\n",
    "valid_data = parser.valid_data\n",
    "hidden_size = parser.n_hidden\n",
    "n_epochs = parser.n_epochs\n",
    "learning_rate = parser.learning_rate\n",
    "n_filters = parser.n_filters\n",
    "filter_size = parser.filter_size\n",
    "savepath = parser.savepath\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e71e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test part\n",
    "parser = ArgumentParser(description=\"FFNN train script\")\n",
    "parser.add_argument(\"-train\", action=\"store\", dest=\"test_data\", type=str, help=\"File with peptides (pep target)\")\n",
    "parser.add_argument(\"-nh\", action=\"store\", dest=\"n_hidden\", type=int, default=16, help=\"Number of hidden units\")\n",
    "parser.add_arguemnt(\"-savepath\", action=\"store\", dest=\"savepath\", type=str, default='./CustomName', help='Path to save the result. Used to load the model as {savepath}_ffnn_model.pkl and save the predictions as {savepath}_ffnn_predictions.txt ; Must not have an extension, ex : ./path/to/my_file')\n",
    "args = parser.parse_args()\n",
    "test_data = parser.test_data\n",
    "hidden_size = parser.n_hidden\n",
    "savepath = parser.savepath"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
